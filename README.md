# ğŸ† NeurIPS 2025 - Open Polymer Prediction Challenge

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-green.svg)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)


## ğŸš€ State-of-the-Art Multi-Task Polymer Property Prediction

This repository contains our cutting-edge solution for the **NeurIPS 2025 Open Polymer Prediction Challenge**. Our approach leverages the latest advances in graph neural networks, transformer architectures, and multi-modal learning to predict five critical polymer properties from SMILES representations.

## ğŸ“Š Challenge Overview

**Objective**: Predict 5 polymer properties from chemical structures (SMILES)
- **Tg** (Glass Transition Temperature)
- **FFV** (Fractional Free Volume)
- **Tc** (Crystallization Temperature)
- **Density**
- **Rg** (Radius of Gyration)

**Evaluation Metric**: Weighted Mean Absolute Error (MAE) with property-specific weights

## ğŸŒŸ Key Features

### ğŸ§  Advanced Neural Architecture
- **Graph Transformer Networks** with multi-head attention mechanisms
- **Heterogeneous GNN Ensemble**: GAT, GIN, SAGE, GCN, ChebNet
- **Pre-trained ChemBERTa** transformer for SMILES encoding
- **Uncertainty Quantification** with aleatoric uncertainty estimation

### ğŸ”¬ Comprehensive Feature Engineering
- **1600+ Mordred Molecular Descriptors**
- **3D Conformer-based Features** (when applicable)
- **Multiple Molecular Fingerprints**:
  - Morgan (Extended Connectivity)
  - MACCS Keys
  - RDKit Fingerprint
  - Atom Pairs
  - Topological Torsion
- **Graph-level Features**: Wiener Index, Balaban J, Graph Energy

### ğŸ¤– Machine Learning Ensemble
- **XGBoost** with custom objectives
- **LightGBM** with categorical features
- **CatBoost** with GPU acceleration (when available)
- **Random Forest** & **Extra Trees** for stability
- **Stacking Meta-Learner** for optimal combination

### ğŸ’¡ Intelligent Data Handling
- **Smart Imputation Module**: Chemical similarity-based imputation
- **Structure-Property Relationships**: Physics-informed predictions
- **Masked Loss Functions**: Proper handling of 93.6% missing Tg values
- **Weighted MAE**: Adaptive weighting based on data availability

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/neurips-polymer-prediction-2025.git
cd neurips-polymer-prediction-2025

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## ğŸ“ Project Structure

```
neurips-polymer-prediction-2025/
â”œâ”€â”€ polymer_prediction_sota_final.py   # Main training and prediction script
â”œâ”€â”€ smart_imputation.py                # Intelligent imputation strategies
â”œâ”€â”€ requirements.txt                   # Python dependencies
â”œâ”€â”€ README.md                         # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ dataset1.csv                  # Training data (874 samples)
â”‚   â”œâ”€â”€ dataset2.csv                  # Training data (7208 samples)
â”‚   â”œâ”€â”€ dataset3.csv                  # Training data (46 samples)
â”‚   â”œâ”€â”€ dataset4.csv                  # Test data (862 samples)
â”‚   â””â”€â”€ submission_sample.csv         # Submission format
â”œâ”€â”€ cache/                            # Cached features and models
â”œâ”€â”€ checkpoints/                      # Model checkpoints
â””â”€â”€ submissions/                      # Generated predictions
```

## ğŸš€ Usage

### Training and Prediction

```bash
# Run the complete pipeline
python polymer_prediction_sota_final.py

# The script will:
# 1. Load and preprocess data
# 2. Extract molecular features (cached for efficiency)
# 3. Train multi-architecture GNN ensemble
# 4. Train ML ensemble models
# 5. Generate predictions with uncertainty estimates
# 6. Apply intelligent imputation for missing predictions
# 7. Create submission file
```

### Configuration Options

Edit the `Config` class in `polymer_prediction_sota_final.py`:

```python
class Config:
    # Model architecture
    gnn_hidden_dim = 512
    gnn_layers = 6
    num_heads = 8
    dropout = 0.2
    
    # Training
    batch_size = 32
    epochs = 100
    learning_rate = 1e-3
    weight_decay = 1e-5
    
    # K-fold cross-validation
    n_folds = 5
    
    # Feature engineering
    use_3d_features = True
    use_mordred = True
    use_transformers = True
```

## ğŸ”¬ Technical Approach

### 1. Multi-Modal Feature Extraction
- **Molecular Graphs**: Node features from atomic properties, edge features from bond types
- **SMILES Sequences**: Tokenized and embedded using pre-trained ChemBERTa
- **Descriptors**: Comprehensive set of physicochemical properties
- **Fingerprints**: Multiple representations for similarity computation

### 2. Graph Neural Network Architecture
```
Input SMILES â†’ Molecular Graph â†’ Multi-Architecture GNN Layers â†’ 
â†’ Graph Pooling â†’ Feature Fusion â†’ Property-Specific Heads â†’ Predictions
```

### 3. Ensemble Strategy
- **Level 1**: Multiple GNN architectures with different inductive biases
- **Level 2**: Gradient boosting ensemble on molecular features
- **Level 3**: Weighted combination based on per-property performance

### 4. Missing Data Strategy
- **Training**: Masked loss functions ignore missing values
- **Inference**: Smart imputation using:
  - Chemical similarity (Tanimoto distance)
  - Structure-property relationships
  - Multi-variate imputation with property correlations

## ğŸ“ˆ Performance

| Property | Data Coverage | OOF MAE | Test MAE |
|----------|--------------|---------|----------|
| Tg       | 3.3%         | TBD     | TBD      |
| FFV      | 46.5%        | TBD     | TBD      |
| Tc       | 4.3%         | TBD     | TBD      |
| Density  | 3.6%         | TBD     | TBD      |
| Rg       | 3.6%         | TBD     | TBD      |

## ğŸ§ª Reproducibility

- **Random Seeds**: Fixed for PyTorch, NumPy, and random modules
- **Feature Caching**: Deterministic feature extraction
- **Checkpointing**: Best models saved per fold
- **Environment**: Tested on Python 3.8-3.11, PyTorch 2.0+

## ğŸ“š References

1. **MMPolymer**: Multi-modal representation learning for polymer property prediction (2024)
2. **Graph Transformers**: Generalization of transformers to graph-structured data
3. **ChemBERTa**: Pre-trained transformer models for molecular property prediction
4. **Mordred**: A molecular descriptor calculator

## ğŸ¤ Contributing

We welcome contributions! Please see our contributing guidelines for details.

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ† Competition Details

- **Competition**: NeurIPS 2025 - Open Polymer Prediction Challenge
- **Prize Pool**: $1,000,000
- **Evaluation**: Weighted MAE across 5 polymer properties
- **Timeline**: TBD

## ğŸ’» Hardware Requirements

- **Minimum**: 16GB RAM, 4-core CPU
- **Recommended**: 32GB RAM, 8-core CPU, GPU (optional but speeds up CatBoost)
- **Storage**: ~10GB for features and model checkpoints

## ğŸ› Known Issues

- 3D conformer generation may fail for some complex polymers
- Memory usage can be high with full Mordred descriptor set
- First run will be slow due to feature extraction

## ğŸ“ Contact

For questions or collaborations:
- Create an issue in this repository
- Email: lorenzo.detomasi@outlook.com

---

**Note**: This solution represents state-of-the-art techniques as of 2025, incorporating the latest research in graph neural networks, molecular representation learning, and multi-task learning for polymer property prediction.
